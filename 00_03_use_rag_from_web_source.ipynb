{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記のリンクをクリックするとGoogle Colabで実行することが出来ます  \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/2Nike2/LangChainPractice/blob/main/00_03_use_retrieval_from_web_source.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (事前準備: OpenAI APIキーの設定)\n",
    "OpenAI APIを使う為のAPIキーを設定します  \n",
    "このAPIキーについては、OpenAIのサイトで取得することが出来ます  \n",
    "https://platform.openai.com/api-keys  \n",
    "APIキーについては公開しないように注意してください  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ここにあなたのOpenAIのAPIキーを入力してください\n",
    "openai_api_key = 'yourapikey'\n",
    "\n",
    "# 環境変数にAPIキーがまだ設定されていないならばAPIキーを設定\n",
    "if os.getenv('OPENAI_API_KEY') is None:\n",
    "    os.environ['OPENAI_API_KEY'] = openai_api_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web文書をソースとしたRAG\n",
    "検索した文書を指示、質問と共にコンテキストとしてLLMに渡すことで正確な回答を得る手法RAG(Retrieval Augmented Generation:検索強化生成)の動作を確認します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインストール\n",
    "LangChainのライブラリをインストールします  \n",
    "またWeb文書を取得するためのライブラリであるBeautifulSoup  \n",
    "及びベクトルデータベースのfaiss(CPU版)もインストールします  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-openai==0.1.4\n",
    "!pip install langchain-openai==0.0.5\n",
    "!pip install beautifulsoup4==4.12.3\n",
    "!pip install faiss-cpu==1.7.4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの初期化\n",
    "OpenAI APIを使う為のモデルを初期化します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEb文書の取得\n",
    "WebBaseLoaderを使ってWeb文書を取得します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://www.aozora.gr.jp/cards/001095/files/42921_23109.html\") # 青空文庫 反スタイルの記」(坂口安吾)\n",
    "\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 埋め込みモデル\n",
    "埋め込みモデルを初期化します\n",
    "これは今まで使ってきた文章に対して文章を返すモデルと違い、文章に対して埋め込みベクトル(数百~数千個の数値のリスト)を返すモデルです  \n",
    "個の埋め込みベクトルというのは文章の意味を数値に要約したものと考えられ、複数の埋め込みベクトルの距離や角度を計算することによって文章の意味の近さを捉えられます  \n",
    "これを利用して指示や質問に対して意味が近かったり関連性が高い文章を探し出すことが出来ます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベクトルデータベースの用意\n",
    "上記の埋め込みベクトルを格納して高速に検索するためのベクトルデータベースを用意します  \n",
    "ベクトルデータベースに登録する文章は全体をそのまま入れるのではなく、何らかの単位(字数、章、ページ区切り等)で分割してからベクトル化して保存することになります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter()\n",
    "text_splitter = CharacterTextSplitter(separator='。', chunk_size=500, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### チェインを作成\n",
    "質問への回答をしてもらう為のチェインを作成します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import  ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "以下の質問について、与えられたcontextを元に回答してください。\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "質問: {input}\n",
    "\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文脈を直接与えて回答\n",
    "上記のチェインで質問と文脈を直接与えたときの挙動を確認します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "print(document_chain.invoke({\n",
    "    'input': 'ヒロポンで眠れないときにはどう対処しているか端的に答えて。',\n",
    "    'context': [Document(page_content=\"\"\"\\\n",
    "ヒロポンを用いて仕事をすると、三日や四日の徹夜ぐらい平気の代りに、いざ仕事が終って眠りたいという時に、眠ることができない。\n",
    "眠るためには酒を飲む必要があり、ヒロポンの効果を消して眠るまでには多量の酒が必要で、ウイスキーを一本半か二本飲む必要がある。\n",
    "原稿料がウイスキーで消えてなくなり足がでるから、バカげた話で、私は要するに、全然お金をもうけていないのである。\\\n",
    "\"\"\")]\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文章を検索して文脈を取得して回答\n",
    "上記の方法だと直接人手で与えた文章を使っている為、  \n",
    "今度はRAGで文章を検索して文脈を取得し、正しく回答できることを確認します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever(search_kwargs={'k': 1})\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "response = retrieval_chain.invoke({'input': '新美さんが勉強していた分野は？'})\n",
    "\n",
    "pprint(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
