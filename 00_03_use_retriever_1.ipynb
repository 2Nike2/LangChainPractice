{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記のリンクをクリックするとGoogle Colabで実行することが出来ます  \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/2Nike2/LangChainPractice/blob/main/00_03_use_retriever.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (事前準備: OpenAI APIキーの設定)\n",
    "OpenAI APIを使う為のAPIキーを設定します  \n",
    "このAPIキーについては、OpenAIのサイトで取得することが出来ます  \n",
    "https://platform.openai.com/api-keys  \n",
    "APIキーについては公開しないように注意してください  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ここにあなたのOpenAIのAPIキーを入力してください\n",
    "openai_api_key = 'yourapikey'\n",
    "\n",
    "# 環境変数にAPIキーがまだ設定されていないならばAPIキーを設定\n",
    "if os.getenv('OPENAI_API_KEY') is None:\n",
    "    os.environ['OPENAI_API_KEY'] = openai_api_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web文書をソースとしたリトリーバを使ったRAG\n",
    "検索した文書を指示、質問と共にコンテキストとしてLLMに渡すことで正確な回答を得る手法RAG(Retrieval Augmented Generation:検索強化生成)の動作を確認します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインストール\n",
    "LangChainのライブラリをインストールします  \n",
    "またWeb文書を取得するためのライブラリであるBeautifulSoup  \n",
    "及びベクトルデータベースのfaiss(CPU版)もインストールします  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.1.4\n",
    "!pip install langchain-openai==0.0.5\n",
    "!pip install beautifulsoup4==4.12.3\n",
    "!pip install faiss-cpu==1.7.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの初期化\n",
    "OpenAI APIを使う為のモデルを初期化します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web文書の取得\n",
    "WebBaseLoaderを使ってWeb文書を取得します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://www.aozora.gr.jp/cards/000081/files/43754_17659.html\") # 青空文庫 「注文の多い料理店」(宮沢賢治)\n",
    "\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 埋め込みモデル\n",
    "埋め込みモデルを初期化します\n",
    "これは今まで使ってきた文章に対して文章を返すモデルと違い、文章に対して埋め込みベクトル(数百~数千個の数値のリスト)を返すモデルです  \n",
    "個の埋め込みベクトルというのは文章の意味を数値に要約したものと考えられ、複数の埋め込みベクトルの距離や角度を計算することによって文章の意味の近さを捉えられます  \n",
    "これを利用して指示や質問に対して意味が近かったり関連性が高い文章を探し出すことが出来ます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベクトルデータベースの用意\n",
    "上記の埋め込みベクトルを格納して高速に検索するためのベクトルデータベースを用意します  \n",
    "ベクトルデータベースに登録する文章は全体をそのまま入れるのではなく、何らかの単位(字数、章、ページ区切り等)で分割してからベクトル化して保存することになります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter()\n",
    "text_splitter = CharacterTextSplitter(separator='\\n', chunk_size=800, chunk_overlap=100)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### チェインを作成\n",
    "質問への回答をしてもらう為のチェインを作成します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import  ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "以下の質問について、与えられたcontextを元に回答してください。\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "質問: {input}\n",
    "\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文脈を直接与えて回答\n",
    "上記のチェインで質問と文脈を直接与えたときの挙動を確認します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "print(document_chain.invoke({\n",
    "    'input': '玄関の札に出ていたレストランの日本語名は？',\n",
    "    'context': [Document(page_content=\"\"\"\\\n",
    "ところがどうも困ったことは、どっちへ行けば戻れるのか、いっこうに見当がつかなくなっていました。\n",
    "　風がどうと吹いてきて、草はざわざわ、木の葉はかさかさ、木はごとんごとんと鳴りました。\n",
    "「どうも腹が空いた。さっきから横っ腹が痛くてたまらないんだ。」\n",
    "「ぼくもそうだ。もうあんまりあるきたくないな。」\n",
    "「あるきたくないよ。ああ困ったなあ、何かたべたいなあ。」\n",
    "「喰べたいもんだなあ」\n",
    "　二人の紳士は、ざわざわ鳴るすすきの中で、こんなことを云いました。\n",
    "　その時ふとうしろを見ますと、立派な一軒の西洋造りの家がありました。\n",
    "　そして玄関には\n",
    "\n",
    "RESTAURANT\n",
    "西洋料理店\n",
    "WILDCAT HOUSE\n",
    "山猫軒\n",
    "\n",
    "という札がでていました。\n",
    "「君、ちょうどいい。ここはこれでなかなか開けてるんだ。入ろうじゃないか」\n",
    "「おや、こんなとこにおかしいね。しかしとにかく何か食事ができるんだろう」\n",
    "「もちろんできるさ。看板にそう書いてあるじゃないか」\n",
    "「はいろうじゃないか。ぼくはもう何か喰べたくて倒れそうなんだ。」\n",
    "　二人は玄関に立ちました。玄関は白い瀬戸の煉瓦で組んで、実に立派なもんです。\n",
    "　そして硝子の開き戸がたって、そこに金文字でこう書いてありました。\n",
    "\n",
    "「どなたもどうかお入りください。決してご遠慮はありません」\n",
    "\n",
    "　二人はそこで、ひどくよろこんで言いました。\n",
    "「こいつはどうだ、やっぱり世の中はうまくできてるねえ、きょう一日なんぎしたけれど、こんどはこんないいこともある。このうちは料理店だけれどもただでご馳走するんだぜ。」\n",
    "「どうもそうらしい。決してご遠慮はありませんというのはその意味だ。」\n",
    "　二人は戸を押して、なかへ入りました。そこはすぐ廊下になっていました。その硝子戸の裏側には、金文字でこうなっていました。\\\n",
    "\"\"\")]\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文章を検索して文脈を取得して回答\n",
    "上記の方法だと直接人手で与えた文章を使っている為、  \n",
    "今度はRAGで文章を検索して文脈を取得し、正しく回答できることを確認します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever(search_kwargs={'k': 3})\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "response = retrieval_chain.invoke({'input': '玄関の札に出ていたレストランの日本語名は？'})\n",
    "\n",
    "pprint(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
