{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記のリンクをクリックするとGoogle Colabで実行することが出来ます  \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/2Nike2/LangChainPractice/blob/main/00_03_use_retriever.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (事前準備: OpenAI APIキーの設定)\n",
    "OpenAI APIを使う為のAPIキーを設定します  \n",
    "このAPIキーについては、OpenAIのサイトで取得することが出来ます  \n",
    "https://platform.openai.com/api-keys  \n",
    "APIキーについては公開しないように注意してください  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ここにあなたのOpenAIのAPIキーを入力してください\n",
    "openai_api_key = 'yourapikey'\n",
    "\n",
    "# 環境変数にAPIキーがまだ設定されていないならばAPIキーを設定\n",
    "if os.getenv('OPENAI_API_KEY') is None:\n",
    "    os.environ['OPENAI_API_KEY'] = openai_api_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web文書をソースとしたリトリーバを使ったRAG\n",
    "検索した文書を指示、質問と共にコンテキストとしてLLMに渡すことで正確な回答を得る手法RAG(Retrieval Augmented Generation:検索強化生成)の動作を確認します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインストール\n",
    "LangChainのライブラリをインストールします  \n",
    "またWeb文書を取得するためのライブラリであるBeautifulSoup  \n",
    "及びベクトルデータベースのfaiss(CPU版)もインストールします  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.1.4\n",
    "!pip install langchain-openai==0.0.5\n",
    "!pip install beautifulsoup4==4.12.3\n",
    "!pip install faiss-cpu==1.7.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの初期化\n",
    "OpenAI APIを使う為のモデルを初期化します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web文書の取得\n",
    "WebBaseLoaderを使ってWeb文書を取得します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://www.aozora.gr.jp/cards/000081/files/43754_17659.html\") # 青空文庫 「注文の多い料理店」(宮沢賢治)\n",
    "\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 埋め込みモデル\n",
    "埋め込みモデルを初期化します\n",
    "これは今まで使ってきた文章に対して文章を返すモデルと違い、文章に対して埋め込みベクトル(数百~数千個の数値のリスト)を返すモデルです  \n",
    "個の埋め込みベクトルというのは文章の意味を数値に要約したものと考えられ、複数の埋め込みベクトルの距離や角度を計算することによって文章の意味の近さを捉えられます  \n",
    "これを利用して指示や質問に対して意味が近かったり関連性が高い文章を探し出すことが出来ます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベクトルデータベースの用意\n",
    "上記の埋め込みベクトルを格納して高速に検索するためのベクトルデータベースを用意します  \n",
    "ベクトルデータベースに登録する文章は全体をそのまま入れるのではなく、何らかの単位(字数、章、ページ区切り等)で分割してからベクトル化して保存することになります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter()\n",
    "text_splitter = CharacterTextSplitter(separator='\\n', chunk_size=800, chunk_overlap=100)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### リトリーバの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever(search_kwargs={'k': 3})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 会話履歴リトリーバ\n",
    "会話履歴を考慮したリトリーバを作ります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt_retriever_query = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    ('user', '{input}'),\n",
    "    ('user', '上記の会話について、関係する情報を取得できる様な検索クエリを作成して下さい。'),\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt_retriever_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 会話履歴リトリーバの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content='ガラスのツボに入っていたものは？'), \n",
    "    AIMessage(content='壺のなかのものは牛乳のクリームでした。')\n",
    "    ]\n",
    "question = 'それをどの様にして使いましたか？'\n",
    "\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": question\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "prompt_to_question = ChatPromptTemplate.from_messages([\n",
    "    ('system', '以下の文脈に基づいてユーザの質問に答えて下さい。\\n\\n{context}'),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    ('user', '{input}')\n",
    "])\n",
    "\n",
    "# prompt_to_question = ChatPromptTemplate.from_messages([\n",
    "#     SystemMessage(content='以下の文脈に基づいてユーザの質問に答えて下さい。\\n\\n{context}'),\n",
    "#     MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "#     HumanMessage(content='{input}')\n",
    "# ])\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt_to_question)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)\n",
    "\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": question\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (参考) 文脈を与えずに同じ質問をした場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": [],\n",
    "    \"input\": question\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
