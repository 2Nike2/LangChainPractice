{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記のリンクをクリックするとGoogle Colabで実行することが出来ます  \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/2Nike2/LangChainPractice/blob/main/01_Get_started/00_00_basic_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (事前準備: OpenAI APIキーの設定)\n",
    "OpenAI APIを使う為のAPIキーを設定します  \n",
    "このAPIキーについては、OpenAIのサイトで取得することが出来ます  \n",
    "https://platform.openai.com/api-keys  \n",
    "APIキーについては公開しないように注意してください  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ここにあなたのOpenAIのAPIキーを入力してください\n",
    "openai_api_key = 'yourapikey'\n",
    "\n",
    "# 環境変数にAPIキーがまだ設定されていないならばAPIキーを設定\n",
    "if os.getenv('OPENAI_API_KEY') is None:\n",
    "    os.environ['OPENAI_API_KEY'] = openai_api_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本例\n",
    "プロンプト+モデル+出力パーサからなる基本的な使い方を確認します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインストール\n",
    "LangChainのライブラリをインストールします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.1.4\n",
    "!pip install langchain-openai==0.0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### プロンプト\n",
    "LLMに入力する文章です。  \n",
    "フォーマットを利用して指定の部分を置き換えたりすることもできます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('{topic}についての短いジョークを話して。')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体的なプロンプトを得るには、フォーマットで使われている変数を辞書として渡します。  \n",
    "プロンプトは、文字列を入力とするLLMとやりとりのメッセージのリストを入力とするChatModelの両方に対応できる様になっており、  \n",
    "基本的にどちらに渡すかを考慮して書き分ける必要がなくなっています。  \n",
    "(下記では確認のため、to_string、to_messageの出力を見ています。)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_value = prompt.invoke({'topic': 'アイスクリーム'})\n",
    "\n",
    "print('文字列')\n",
    "print(prompt_value.to_string())\n",
    "print('メッセージのリスト')\n",
    "print(prompt_value.to_messages())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル\n",
    "モデルを用意します。  \n",
    "ここではメッセージのリストを入力とする```ChatModel```の代表としてgpt-4、  \n",
    "文字列を入力とする```LLM```の代表としてgpt-3.5-turbo-instruct  \n",
    "を用意します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4')\n",
    "\n",
    "llm =OpenAI(model='gpt-3.5-turbo-instruct')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```ChatModel```、```LLM```を単独で使ってみると下記の様になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ChatModel')\n",
    "print(model.invoke('アイスクリームについての短いジョークを話して。'))\n",
    "print('-'*50)\n",
    "print('LLM')\n",
    "print(llm.invoke('アイスクリームについての短いジョークを話して。'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出力パーサ\n",
    "出力を整えるパーサを用意します。  \n",
    "StrOutputParserは入力を文字列に直します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力パーサを単独で使用したとき、  \n",
    "文字列を入れたらそのまま文字列が返ってきて、  \n",
    "```AIMessage```を入れた時も文字列が返ってくることが確認できます。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.ai import AIMessage\n",
    "\n",
    "print(output_parser.invoke('元文字列'))\n",
    "print('-'*50)\n",
    "\n",
    "ai_message = AIMessage('元メッセージ')\n",
    "\n",
    "print('メッセージをそのまま出力')\n",
    "print(ai_message)\n",
    "print('-'*25)\n",
    "print('出力パーサ利用')\n",
    "print(output_parser.invoke(ai_message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## チェインの作成、実行\n",
    "上記で用意したプロンプト、LLM、出力パーサをつなげて実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({'topic': 'アイスクリーム'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
