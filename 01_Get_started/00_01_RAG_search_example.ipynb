{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記のリンクをクリックするとGoogle Colabで実行することが出来ます  \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/2Nike2/LangChainPractice/blob/main/01_Get_started/00_01_RAG_search_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (事前準備: OpenAI APIキーの設定)\n",
    "OpenAI APIを使う為のAPIキーを設定します  \n",
    "このAPIキーについては、OpenAIのサイトで取得することが出来ます  \n",
    "https://platform.openai.com/api-keys  \n",
    "APIキーについては公開しないように注意してください  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ここにあなたのOpenAIのAPIキーを入力してください\n",
    "openai_api_key = 'yourapikey'\n",
    "\n",
    "# 環境変数にAPIキーがまだ設定されていないならばAPIキーを設定\n",
    "if os.getenv('OPENAI_API_KEY') is None:\n",
    "    os.environ['OPENAI_API_KEY'] = openai_api_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n",
    "検索強化生成の方法を確認します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインストール\n",
    "LangChainのライブラリをインストールします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.1.4\n",
    "!pip install langchain-openai==0.0.5\n",
    "!pip install docarray==0.32.1\n",
    "!pip install tiktoken=0.5.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 埋め込みモデル\n",
    "文章を意味を凝縮した形で保存できるベクトルに変換する埋め込みモデルを用意します。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベクトルストア、リトリーバの作成\n",
    "文章をベクトル化して保存するデータストア(ベクトルストア)  \n",
    "及びそれを対象として検索を行うためのリトリーバを作成します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    ['太郎は区役所で働いています。', 'クマは蜂蜜を食べるのが好きです。'],\n",
    "    embedding=embedding\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単独で実行すると入力した質問に関係のある文章が返ってきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上位1件の文章を返す。\n",
    "retriever.invoke('太郎はどこで働いていますか？')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テンプレート\n",
    "テンプレートを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template('''\\\n",
    "下記の文脈に基づいて質問に答えて下さい:\n",
    "{context}\n",
    "質問:{question}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル\n",
    "モデルを用意します。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出力パーサ\n",
    "出力を整えるパーサを用意します。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 質問と文脈用意の並列処理\n",
    "質問を文脈を後の処理に同時に渡せるようにしています。  \n",
    "質問に関連する文脈を渡しつつ、同時に元の質問を後の処理に受け渡しています。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {'context': retriever, 'question': RunnablePassthrough()}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## チェインの作成、実行\n",
    "上記で用意した各種要素をつなげて実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke('太郎はどこで働いていますか？')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
